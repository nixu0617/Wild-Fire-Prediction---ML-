{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "HXY7I_Jm3SMA",
        "TnpY5qVHuE9x",
        "gGOydEOH6tce",
        "ul4FPGl57I4P"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MapleWolfe/Milestone_2/blob/main/Unsupervised_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning techniques"
      ],
      "metadata": {
        "id": "IuuckSotcRHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## installs, imports, pre-sets"
      ],
      "metadata": {
        "id": "Uv1syv3gdvb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "#!python rapidsai-csp-utils/colab/pip-install.py\n",
        "#!pip install google-cloud-storage"
      ],
      "metadata": {
        "id": "qudk0RcgDLyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_fG6RA56XD_4"
      },
      "outputs": [],
      "source": [
        "#google import options\n",
        "#from google.colab import drive\n",
        "from google.cloud import storage\n",
        "\n",
        "#general usage imports\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import gc\n",
        "import os\n",
        "import multiprocessing\n",
        "import pickle\n",
        "import json\n",
        "import joblib\n",
        "#clustering import\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "#PCA imports\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import OPTICS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dask set up"
      ],
      "metadata": {
        "id": "HXY7I_Jm3SMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cpus = multiprocessing.cpu_count()\n",
        "print(\"Number of available CPUs:\", num_cpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raB23_8jCqUs",
        "outputId": "64713b93-796d-4124-9cbf-4ca133d878aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of available CPUs: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP set up"
      ],
      "metadata": {
        "id": "TnpY5qVHuE9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/organic-reef-390716-609989a4c6da.json'\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket('fire_train_eval_test_bucket')\n",
        "blob = bucket.blob('test.csv')\n",
        "blob.download_to_filename('test.csv')\n",
        "blob = bucket.blob('eval.csv')\n",
        "blob.download_to_filename('eval.csv')\n",
        "blob = bucket.blob('train.csv')\n",
        "blob.download_to_filename('train.csv')"
      ],
      "metadata": {
        "id": "Wi1hP1Hbs7sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to load csv chunks"
      ],
      "metadata": {
        "id": "B_yF4lBdd2_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's mount the drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rG2_aSl8ArTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remember to add .csv at the end of file name\n",
        "def read_csv_in_chunks(file_name,number_images):\n",
        "\n",
        "  #number of rows per image:\n",
        "  pixels_count = 64*64\n",
        "\n",
        "  #upto 200 images at a time\n",
        "  size = number_images*pixels_count\n",
        "\n",
        "  #file string and location for Google Drive\n",
        "  #file_string = '/content/drive/MyDrive/' + file_name\n",
        "\n",
        "  #file string and location for Google cloud storage\n",
        "  file_string = '/content/' + file_name\n",
        "  return pd.read_csv(file_string, chunksize=size)\n",
        "\n",
        "def read_full_csv(file_name):\n",
        "  #file string and location for Google Drive\n",
        "  #file_string = '/content/drive/MyDrive/' + file_name\n",
        "\n",
        "  #file string and location for Google cloud storage\n",
        "  file_string = '/content/' + file_name\n",
        "\n",
        "\n",
        "  return pd.read_csv(file_string)"
      ],
      "metadata": {
        "id": "wwtDvsYWPRUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to clean CSV chunks"
      ],
      "metadata": {
        "id": "TmldQYHo52YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is procedure that cleans the data.\n",
        "# cleaner_1 drops all negative \"firemask\" values and converts all values above 0 to 1\n",
        "def cleaner_1(df_chunk):\n",
        "  col_list = ['NDVI_scaled_smoothened_values', 'NDVI_local_gradient', 'NDVI_local_mean', 'tmmn_scaled_smoothened_values', 'tmmn_local_gradient', 'tmmn_local_mean', 'elevation_scaled_smoothened_values', 'elevation_local_gradient', 'elevation_local_mean', 'fire_at_similar_altitude', 'population_scaled_smoothened_values', 'population_local_gradient', 'population_local_mean', 'vs_scaled_smoothened_values', 'vs_local_gradient', 'vs_local_mean', 'pdsi_scaled_smoothened_values', 'pdsi_local_gradient', 'pdsi_local_mean', 'pr_scaled_smoothened_values', 'pr_local_gradient', 'pr_local_mean', 'tmmx_scaled_smoothened_values', 'tmmx_local_gradient', 'tmmx_local_mean', 'sph_scaled_smoothened_values', 'sph_local_gradient', 'sph_local_mean', 'th_scaled_smoothened_values', 'th_local_gradient', 'th_local_mean', 'distance_from_fire', 'erc_scaled_smoothened_values', 'erc_local_gradient', 'erc_local_mean']\n",
        "\n",
        "  original_previous_day_fire = df_chunk['PrevFireMask']\n",
        "  original_next_day_fire = df_chunk['FireMask']\n",
        "\n",
        "  #general cleaning for classifier and regressor\n",
        "  drop_neg_df = df_chunk[df_chunk['FireMask'] >=0]\n",
        "\n",
        "  #only regressor selection\n",
        "  regressor_target = drop_neg_df['FireMask']\n",
        "\n",
        "  #cleaning specifically for the classifier\n",
        "  classifier_target = np.where(regressor_target > 0, 1, 0)\n",
        "  dropped_chunk = df_chunk.drop(labels=['PrevFireMask','FireMask','image_id'], axis=1)\n",
        "  output_chunk = dropped_chunk[col_list]\n",
        "  return output_chunk,regressor_target,classifier_target, original_previous_day_fire, original_next_day_fire"
      ],
      "metadata": {
        "id": "JoAEDwdU51eF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = read_full_csv('train.csv')\n",
        "print('train loaded')\n",
        "cleaned_df,_,_,_,_ = cleaner_1(train_df)\n",
        "print('initializing data scaling')\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(cleaned_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3c7aV7DLAUe",
        "outputId": "a2f64e91-d770-49cf-d9be-027290f64dfa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loaded\n",
            "initializing data scaling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del train_df, cleaned_df\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "mK9rYGyFRggq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Learning"
      ],
      "metadata": {
        "id": "nmWk-aGU6XXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PCA"
      ],
      "metadata": {
        "id": "prRulC-wc2ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_param_grid = {'n_components': [4,6,8,10,12,14,16,18]}\n",
        "pca_storage_dict = {}\n",
        "model_counter = 0\n",
        "for params in ParameterGrid(pca_param_grid):\n",
        "  model_counter +=1\n",
        "  print('initializing PCA for param: ', params)\n",
        "  pca_model = PCA(**params)\n",
        "  chunk_counter = 0\n",
        "  pca_model.fit(data_scaled)\n",
        "  print('pca completed for model : ', model_counter)\n",
        "\n",
        "  pca_model_name_string = 'pca_model_'+str(model_counter)\n",
        "  pca_storage_dict[pca_model_name_string] = [params,\n",
        "                                             {'explained_variance': list(pca_model.explained_variance_)},\n",
        "                                             {'explained_variance_ratio':list(pca_model.explained_variance_ratio_)}]\n",
        "  print('storing pca file')\n",
        "  with open(pca_model_name_string, 'wb') as pca_file:\n",
        "    pickle.dump(pca_model, pca_file)\n",
        "\n",
        "print('storing scalar model')\n",
        "with open('standard_scalar_model', 'wb') as scaler_file:\n",
        "  pickle.dump(scaler, scaler_file)\n",
        "\n",
        "print('storing pca performance')\n",
        "with open('pca_model_performance.json', 'w') as pca_metric_json:\n",
        "    json.dump(pca_storage_dict, pca_metric_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bes4O_IeVUor",
        "outputId": "655f4d99-fb7d-4614-9609-d731f06815de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing PCA for param:  {'n_components': 4}\n",
            "pca completed for model :  1\n",
            "storing pca file\n",
            "initializing PCA for param:  {'n_components': 6}\n",
            "pca completed for model :  2\n",
            "storing pca file\n",
            "initializing PCA for param:  {'n_components': 8}\n",
            "pca completed for model :  3\n",
            "storing pca file\n",
            "initializing PCA for param:  {'n_components': 10}\n",
            "pca completed for model :  4\n",
            "storing pca file\n",
            "initializing PCA for param:  {'n_components': 12}\n",
            "pca completed for model :  5\n",
            "storing pca file\n",
            "initializing PCA for param:  {'n_components': 14}\n",
            "pca completed for model :  6\n",
            "storing pca file\n",
            "initializing PCA for param:  {'n_components': 16}\n",
            "pca completed for model :  7\n",
            "storing pca file\n",
            "initializing PCA for param:  {'n_components': 18}\n",
            "pca completed for model :  8\n",
            "storing pca file\n",
            "storing scalar model\n",
            "storing pca performance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Kmeans Clustering"
      ],
      "metadata": {
        "id": "Xt1apEuP6qRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open pca model:\n",
        "with open('/content/pca_model_8', 'rb') as pca_file:\n",
        "    loaded_pca_model = pickle.load(pca_file)"
      ],
      "metadata": {
        "id": "gqvXkKKko6m7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_evaluation(eval_df, cluster_model):\n",
        "    print('evaluation start')\n",
        "    eval_labels = cluster_model.predict(eval_df)\n",
        "    inertia = cluster_model.inertia_\n",
        "    calinski = calinski_harabasz_score(eval_df, eval_labels)\n",
        "    davies_bouldin = davies_bouldin_score(eval_df, eval_labels)\n",
        "    # we are no longer calculating silhouette score as it performs pairwise calculations that grow exponentially with data\n",
        "    #silhouette = silhouette_score(eval_df, eval_labels)\n",
        "    print('evaluation complete')\n",
        "    return inertia, calinski, davies_bouldin"
      ],
      "metadata": {
        "id": "fqXUlzjO5j7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets build a function for our kmeans cluster\n",
        "def search_params_kmeans(file,scaling_model,pca_model,cluster_list=[8,32,64],initialisation_list = ['k-means++', 'random'], random_state = [0]):\n",
        "  k_means_param_grid = {'n_clusters': cluster_list, 'init': initialisation_list, 'random_state' : random_state, 'batch_size' : [1024*num_cpus]}\n",
        "  for params in ParameterGrid(k_means_param_grid):\n",
        "    print('initializing kmeans for param: ', params)\n",
        "    csv_chunks_generator = read_csv_in_chunks(file,1000)\n",
        "    K_means_model = MiniBatchKMeans(**params)\n",
        "    counter = 0\n",
        "    for a_chunk in csv_chunks_generator:\n",
        "      features_df,_,_,_,_ = cleaner_1(a_chunk)\n",
        "      scaled_df = scaling_model.transform(features_df)\n",
        "      out_pca_df = pca_model.transform(scaled_df)\n",
        "      K_means_model.partial_fit(out_pca_df)\n",
        "      print('iteration completed: ', counter)\n",
        "      counter+=1\n",
        "    yield K_means_model, params"
      ],
      "metadata": {
        "id": "5ii7V710rRCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# a dict to store model performance & eval csv file\n",
        "print('starting to read evaluation dataset')\n",
        "evaluation_df = read_full_csv('eval.csv')\n",
        "print('read eval')\n",
        "cleaned_eval,_,_,_,_ = cleaner_1(evaluation_df)\n",
        "eval_scaled = scaler.transform(cleaned_eval)\n",
        "eval_pca = loaded_pca_model.transform(eval_scaled)\n",
        "\n",
        "#intermediate memory step\n",
        "print('starting deletion of raw evaluation data')\n",
        "del evaluation_df\n",
        "gc.collect()\n",
        "print('completed deletion of raw evaluation data')\n",
        "\n",
        "# initiating model building\n",
        "model_builders = search_params_kmeans(file='train.csv',scaling_model = scaler, pca_model = loaded_pca_model)\n",
        "model_perform_dict ={}\n",
        "model_counter = 0\n",
        "\n",
        "#this where a lot of time will go, it will iterate over each model across grid search\n",
        "for a_kmean_model, kmean_params in model_builders:\n",
        "  model_counter +=1\n",
        "  print('initializing evaluation')\n",
        "  inertia, calinski, davies_bouldin = cluster_evaluation(eval_pca, a_kmean_model)\n",
        "  print('evaluation complete')\n",
        "\n",
        "  model_name = 'kmean_model_'+str(model_counter)\n",
        "  model_perform_dict[model_name]=[kmean_params,inertia, calinski, davies_bouldin]\n",
        "  print('storing model')\n",
        "  with open(model_name, 'wb') as model_file:\n",
        "    pickle.dump(a_kmean_model, model_file)\n"
      ],
      "metadata": {
        "id": "gT0KhYBp91ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we are just improving the dict\n",
        "for a_key in model_perform_dict.keys():\n",
        "  #inertia, calinski, davies_bouldin\n",
        "  model_perform_dict[a_key][1] = ('inertia',model_perform_dict[a_key][1])\n",
        "  model_perform_dict[a_key][2] = ('calinski',model_perform_dict[a_key][2])\n",
        "  model_perform_dict[a_key][3] = ('davies_bouldin',model_perform_dict[a_key][3])\n",
        "\n",
        "#outputing our evaluation metrics for all the models\n",
        "with open('kmean_model_performance.json', 'w') as kmeans_metric_json:\n",
        "    json.dump(model_perform_dict, kmeans_metric_json)"
      ],
      "metadata": {
        "id": "hcBuqpiAyvVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Birch"
      ],
      "metadata": {
        "id": "zY4h5A5n1TXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('PCA transformation of eval initiating')\n",
        "eval_pca = loaded_pca_model.transform(eval_scaled)\n",
        "\n",
        "print('PCA transformation of train initiating')\n",
        "train_pca = loaded_pca_model.transform(data_scaled)\n",
        "\n",
        "birch_param_grid ={\n",
        "    'threshold': [0.2,0.4,0.6],\n",
        "    'n_clusters': [8,32,64],\n",
        "}\n",
        "\n",
        "birch_performance = {}\n",
        "birch_model_number = 0\n",
        "for params in ParameterGrid(birch_param_grid):\n",
        "  birch_model_number +=1\n",
        "  print('initializing birch model for params: ', params)\n",
        "  birch_model = Birch(**params)\n",
        "  print('fitting birch')\n",
        "  birch_model.fit(train_pca)\n",
        "  print('predicting on birch')\n",
        "  birch_labels = birch_model.predict(eval_pca)\n",
        "  print('evaluating using similar metrics')\n",
        "  calinski = calinski_harabasz_score(eval_pca, birch_labels)\n",
        "  davies_bouldin = davies_bouldin_score(eval_pca, birch_labels)\n",
        "  print('model is complete, now outputing the data')\n",
        "  birch_model_name = 'birch_model_' +str(birch_model_number)\n",
        "  birch_performance[birch_model_name] = {'params':params,'calinski':calinski,'davies_bouldin':davies_bouldin }\n",
        "  print('outputing the model')\n",
        "\n",
        "  with open(birch_model_name, 'wb') as birch_file:\n",
        "    pickle.dump(birch_model, birch_file)\n",
        "\n",
        "with open('bich_model_performance.json', 'w') as birch_metric_json:\n",
        "    json.dump(birch_performance, birch_metric_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnUnlZ-GB0ax",
        "outputId": "0a220e20-37a1-42bf-f6d1-dea55a0e21be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA transformation of eval initiating\n",
            "PCA transformation of train initiating\n",
            "initializing birch model for params:  {'n_clusters': 8, 'threshold': 0.2}\n",
            "fitting birch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jhECfGGRc9t",
        "outputId": "07225365-90ba-4bd1-f98d-99c6d9cfa304"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OPTICS"
      ],
      "metadata": {
        "id": "Cwn_T2ET2wHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pca = loaded_pca_model.transform(data_scaled)\n",
        "\n",
        "\n",
        "optics_param_grid = {\n",
        "    'min_samples': [0.01,0.05,0.1],\n",
        "    'cluster_method': ['xi','dbscan'],\n",
        "    'n_jobs': [-1]\n",
        "}\n",
        "\n",
        "optics_performance = {}\n",
        "optics_model_number = 0\n",
        "for params in ParameterGrid(optics_param_grid):\n",
        "  optics_model_number +=1\n",
        "  print('initializing optics model for params: ', params)\n",
        "  optics_model = OPTICS(**params)\n",
        "  print('fitting optics')\n",
        "  print('predicting on optics')\n",
        "  optics_labels = optics_model.fit_predict(train_pca)\n",
        "  print('evaluating using similar metrics')\n",
        "  calinski = calinski_harabasz_score(eval_pca, optics_labels)\n",
        "  davies_bouldin = davies_bouldin_score(eval_pca, optics_labels)\n",
        "  print('model is complete, now outputing the data')\n",
        "  optic_model_name = 'optic_model_' +str(optics_model_number)\n",
        "  optics_performance[optic_model_name] = {'params':params,'calinski':calinski,'davies_bouldin':davies_bouldin }\n",
        "  print('outputing the model')\n",
        "\n",
        "  with open(optic_model_name, 'wb') as optic_file:\n",
        "    pickle.dump(optics_model, optic_file)\n",
        "\n",
        "with open('optic_model_performance.json', 'w') as optic_metric_json:\n",
        "    json.dump(optics_performance, optic_metric_json)"
      ],
      "metadata": {
        "id": "rgZGqwfR2vlU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4949e4-8bde-4877-9aea-b68ddb777166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing optics model for params:  {'cluster_method': 'xi', 'min_samples': 0.01, 'n_jobs': -1}\n",
            "fitting optics\n",
            "predicting on optics\n"
          ]
        }
      ]
    }
  ]
}