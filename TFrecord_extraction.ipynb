{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MapleWolfe/Milestone_2/blob/Jai/TFrecord_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuuckSotcRHo"
      },
      "source": [
        "# Data extraction from TF records"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv1syv3gdvb9"
      },
      "source": [
        "## installs, imports, pre-sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qudk0RcgDLyx"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_fG6RA56XD_4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import skimage\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import cudf\n",
        "import cupy as cp\n",
        "import gc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_yF4lBdd2_D"
      },
      "source": [
        "## Loading TF records from google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrCeIqHYul-S",
        "outputId": "2cc99179-f62e-45dd-c22d-588ccc18465b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "number of TF records: 19\n",
            "file names of tf records within the zip:\n",
            "['next_day_wildfire_spread_eval_00.tfrecord', 'next_day_wildfire_spread_eval_01.tfrecord', 'next_day_wildfire_spread_test_00.tfrecord', 'next_day_wildfire_spread_test_01.tfrecord', 'next_day_wildfire_spread_train_00.tfrecord', 'next_day_wildfire_spread_train_01.tfrecord', 'next_day_wildfire_spread_train_02.tfrecord', 'next_day_wildfire_spread_train_03.tfrecord', 'next_day_wildfire_spread_train_04.tfrecord', 'next_day_wildfire_spread_train_05.tfrecord', 'next_day_wildfire_spread_train_06.tfrecord', 'next_day_wildfire_spread_train_07.tfrecord', 'next_day_wildfire_spread_train_08.tfrecord', 'next_day_wildfire_spread_train_09.tfrecord', 'next_day_wildfire_spread_train_10.tfrecord', 'next_day_wildfire_spread_train_11.tfrecord', 'next_day_wildfire_spread_train_12.tfrecord', 'next_day_wildfire_spread_train_13.tfrecord', 'next_day_wildfire_spread_train_14.tfrecord']\n"
          ]
        }
      ],
      "source": [
        "# let's mount the drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# let's look into the zip file stored in the google drive\n",
        "wild_fire_file_path = '/content/drive/MyDrive/next_day_wildfire.zip'\n",
        "wildfire_zip =  zipfile.ZipFile(wild_fire_file_path, 'r')\n",
        "tf_record_file_names = wildfire_zip.namelist()\n",
        "\n",
        "print('number of TF records:', len(tf_record_file_names))\n",
        "print('file names of tf records within the zip:')\n",
        "print(tf_record_file_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QuxrZXk6cQEZ"
      },
      "outputs": [],
      "source": [
        "# unzipping one file at a time\n",
        "def one_file_unzip(tf_record_file_name, zipfile_variable):\n",
        "  extracted_record_path = zipfile_variable.extract(tf_record_file_name)\n",
        "  raw_dataset = tf.data.TFRecordDataset(extracted_record_path)\n",
        "  return raw_dataset\n",
        "\n",
        "# yielding out one record at a time\n",
        "def extract_one_row(tf_record_dataset):\n",
        "  for i, raw_record in enumerate(tf_record_dataset.take(tf_record_dataset.cardinality().numpy())):\n",
        "    one_record_dict = {}\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(raw_record.numpy())\n",
        "\n",
        "    for key, feature in example.features.feature.items():\n",
        "\n",
        "      kind = feature.WhichOneof('kind')\n",
        "      one_record_dict[key] = np.array(getattr(feature, kind).value).reshape(64,64)\n",
        "    yield one_record_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XErZ3dz7Hs19"
      },
      "source": [
        "## let's create features from all images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD7ndnzBQ2A1"
      },
      "source": [
        "### feature description given by dataset maker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E4-Gd5OVc94v"
      },
      "outputs": [],
      "source": [
        "# data variables\n",
        "\n",
        "INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph',\n",
        "                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
        "\n",
        "OUTPUT_FEATURES = ['FireMask']\n",
        "\n",
        "\n",
        "# underlying feature value ranges:\n",
        "# (min_clip, max_clip, mean, standard deviation)\n",
        "\n",
        "feature_description_dict = {\n",
        "    # Elevation in m: between 0.1 percentile and 99.9 percentile\n",
        "    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n",
        "\n",
        "    # Palmer Drought Severity Index: between 0.1 percentile and 99.9 percentile\n",
        "    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n",
        "\n",
        "    #Vegetation index times 10,000: between -1 and 1\n",
        "    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),\n",
        "\n",
        "    # Precipitation in mm: between 0.0 and 99.9 percentile\n",
        "    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n",
        "\n",
        "    # Specific humidity: between 0 and 1\n",
        "    'sph': (0., 1., 0.0071658953, 0.0042835088),\n",
        "\n",
        "    # Wind direction in degrees clockwise from north: between 0 and 360.\n",
        "    'th': (0., 360.0, 190.32976, 72.59854),\n",
        "\n",
        "    #Min temp: between 253.15 kelvin and 99.9 percentile\n",
        "    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n",
        "\n",
        "    #Max temp: between 253.15 kelvin and 99.9 percentile\n",
        "    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n",
        "\n",
        "    # Wind speed in m/s: between 0. and 99.9 percentile\n",
        "    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n",
        "\n",
        "    # NFDRS fire danger index energy release component BTU's per square foot.\n",
        "    # 0., 99.9 percentile\n",
        "    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n",
        "\n",
        "    # Population density: between 0 and 99.9 percentile\n",
        "    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n",
        "\n",
        "    # We don't want to normalize the FireMasks.\n",
        "    # 1 indicates fire, 0 no fire, -1 unlabeled data\n",
        "    'PrevFireMask': (-1., 1., 0., 1.),\n",
        "    'FireMask': (-1., 1., 0., 1.)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIZ_dwWPQlRX"
      },
      "source": [
        "### Feature generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i7krqq3gVEpk"
      },
      "outputs": [],
      "source": [
        "# lets define the min max scaling function\n",
        "def min_max_scaling(array,min_val,max_val):\n",
        "    scaled_array = np.clip((array - min_val) / (max_val - min_val), 0, 1)\n",
        "    return scaled_array\n",
        "\n",
        "# let's apply guassian smoothing\n",
        "def gaussian_smoothing(image_array,sigma_val):\n",
        "  smooth_array = skimage.filters.gaussian(image_array, sigma=1)\n",
        "  return smooth_array\n",
        "\n",
        "#lets get the rate of change and mean,\n",
        "def local_pixel_features(image_array,radius_val):\n",
        "  footprint = skimage.morphology.disk(radius_val)\n",
        "  gradient_array = skimage.filters.rank.gradient(image_array, footprint)\n",
        "  mean_array = skimage.filters.rank.mean(image_array, footprint)\n",
        "  return gradient_array,mean_array\n",
        "\n",
        "#use altitude edge to identify whether pixel is at a similar altitude as any pixel that has fire\n",
        "def fire_pixel_shared_altitude(row_dict, normalized_array, previous_day_fire = 'PrevFireMask'):\n",
        "  edges_array = skimage.feature.canny(normalized_array)\n",
        "  inverted_edges_array = np.logical_not(edges_array).astype(int)\n",
        "  edge_label_array = skimage.measure.label(inverted_edges_array)\n",
        "\n",
        "  previous_fire = row_dict[previous_day_fire]\n",
        "  fire_edge_labels = (edge_label_array*previous_fire)\n",
        "\n",
        "  unique_regions_with_fire = np.unique(fire_edge_labels.flatten())\n",
        "  non_zero_unique_regions = unique_regions_with_fire[unique_regions_with_fire != 0]\n",
        "\n",
        "  fire_at_same_altitude = np.isin(edge_label_array, non_zero_unique_regions).astype(int)\n",
        "  return fire_at_same_altitude\n",
        "\n",
        "def distance_to_fire(row_dict,feature):\n",
        "  # we need to clip the fire mask to account for -1 values (missing values where the satellite was unable to get a clear image)\n",
        "  # for now we take them as no fire objects, however we will not be accounting for these pixels in our model.\n",
        "  fire_mask_array = row_dict[feature].clip(0,1)\n",
        "  inverted_mask_array = 1 - fire_mask_array\n",
        "  distance_transform_array = distance_transform_edt(inverted_mask_array)\n",
        "  return distance_transform_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZJKMGL66HcCG"
      },
      "outputs": [],
      "source": [
        "# let's apply it on all features\n",
        "def build_features(record_dict,min_max_dict,sigma_val,radius_val):\n",
        "  feature_list = record_dict.keys()\n",
        "  output_feature_dict = {}\n",
        "  for a_feature in feature_list:\n",
        "    if a_feature not in ['PrevFireMask','FireMask']:\n",
        "     #min max scaling\n",
        "     feature_min = min_max_dict[a_feature][0]\n",
        "     feature_max = min_max_dict[a_feature][1]\n",
        "     scaled_array = min_max_scaling(record_dict[a_feature],feature_min,feature_max)\n",
        "     #guassian smoothing\n",
        "     smoothen_array = gaussian_smoothing(scaled_array,sigma_val)\n",
        "\n",
        "     #local pixel values: gradient values(rate of change), local mean val.\n",
        "     gradient_array,mean_array = local_pixel_features(smoothen_array,radius_val)\n",
        "\n",
        "     #lets now add these features to our output:\n",
        "     output_feature_dict[a_feature+'_'+'scaled_smoothened_values'] = smoothen_array.flatten()\n",
        "     output_feature_dict[a_feature+'_'+'local_gradient'] = gradient_array.flatten()\n",
        "     output_feature_dict[a_feature+'_'+'local_mean'] = mean_array.flatten()\n",
        "\n",
        "     #lets label pixels if they are at the same elevation (to account for cliffs/mountains/chasms) as the fire\n",
        "     # here we aren't using smoothened array\n",
        "    if a_feature == 'elevation':\n",
        "      fire_at_altitude_array = fire_pixel_shared_altitude(record_dict, scaled_array)\n",
        "      output_feature_dict['fire_at_similar_altitude'] = fire_at_altitude_array.flatten()\n",
        "     #lets move are features into a dict.\n",
        "\n",
        "    # get pixel eucledian distance from fire\n",
        "    if a_feature == 'PrevFireMask':\n",
        "      distance_array = distance_to_fire(record_dict,a_feature)\n",
        "      output_feature_dict['PrevFireMask'] = record_dict[a_feature].flatten()\n",
        "      output_feature_dict['distance_from_fire'] = distance_array.flatten()\n",
        "\n",
        "    if a_feature == 'FireMask':\n",
        "      output_feature_dict['FireMask'] = record_dict[a_feature].flatten()\n",
        "\n",
        "  return output_feature_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J6trc2M_ooIs"
      },
      "outputs": [],
      "source": [
        "def make_df_save_csv(string_file_name,tf_record_names = tf_record_file_names, main_zip_file = wildfire_zip, feature_descriptions= feature_description_dict):\n",
        "  total_rows = 0\n",
        "  image_id = 0\n",
        "  large_df_list = []\n",
        "  column_names = []\n",
        "  csv_file_name = string_file_name + '.csv'\n",
        "  for a_tf_record in tf_record_names:\n",
        "    if string_file_name in a_tf_record:\n",
        "      print('started tf record: ', a_tf_record)\n",
        "      raw_dataset = one_file_unzip(a_tf_record, main_zip_file)\n",
        "      row_extraction_generator = extract_one_row(raw_dataset)\n",
        "      single_record_list = []\n",
        "      image_count = 0\n",
        "\n",
        "      for a_row in row_extraction_generator:\n",
        "        all_features_dict_array = build_features(a_row,feature_descriptions,sigma_val=1,radius_val=3)\n",
        "        column_names = all_features_dict_array.keys()\n",
        "        image_id += 1\n",
        "        image_count +=1\n",
        "        image_number_array = np.full(4096, image_id)\n",
        "        all_features_dict_array['image_id'] = image_number_array\n",
        "\n",
        "        if image_count == 1:\n",
        "          all_features_dataframe = cudf.DataFrame.from_dict(all_features_dict_array)\n",
        "        else:\n",
        "          single_row_df = cudf.DataFrame.from_dict(all_features_dict_array)\n",
        "          all_features_dataframe = all_features_dataframe.append(single_row_df, ignore_index=True)\n",
        "\n",
        "        if image_count >= 200:\n",
        "          single_record_list.append(all_features_dataframe)\n",
        "          image_count = 0\n",
        "\n",
        "      if image_count % 200 != 0:\n",
        "        single_record_list.append(all_features_dataframe)\n",
        "        image_count = 0\n",
        "\n",
        "      big_df = cudf.concat(single_record_list, ignore_index=True)\n",
        "      pandas_big_df = big_df.to_pandas()\n",
        "      total_rows += len(pandas_big_df)\n",
        "      #Headers were being saved on cell info, So I added this check\n",
        "      if '_00' in a_tf_record: #this only works if it runs in order\n",
        "        pandas_big_df.to_csv(csv_file_name, mode='a', index=False, header=True)\n",
        "      else:\n",
        "        pandas_big_df.to_csv(csv_file_name, mode='a', index=False, header=False)\n",
        "      print('completed: ', a_tf_record)\n",
        "\n",
        "  print('csv output is complete')\n",
        "  print('output csv lenght',total_rows)\n",
        "  print('output csv image count: ', total_rows/4096)\n",
        "  print('number of expected images', image_id)\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Duabn-Fvr0T",
        "outputId": "8b49ef0a-2d38-45e7-81ab-84b0cc492358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we are making eval csv\n",
            "started tf record:  next_day_wildfire_spread_eval_00.tfrecord\n",
            "completed:  next_day_wildfire_spread_eval_00.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_eval_01.tfrecord\n",
            "completed:  next_day_wildfire_spread_eval_01.tfrecord\n",
            "csv output is complete\n",
            "output csv lenght 7688192\n",
            "output csv image count:  1877.0\n",
            "number of expected images 1877\n",
            "CPU times: user 5min 25s, sys: 17.8 s, total: 5min 43s\n",
            "Wall time: 6min 11s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "print('we are making eval csv')\n",
        "make_df_save_csv('eval')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS3TsSvmRPU-",
        "outputId": "340c3e80-226e-4fdc-f468-8084b2473e36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we are making eval csv\n",
            "started tf record:  next_day_wildfire_spread_test_00.tfrecord\n",
            "completed:  next_day_wildfire_spread_test_00.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_test_01.tfrecord\n",
            "completed:  next_day_wildfire_spread_test_01.tfrecord\n",
            "csv output is complete\n",
            "output csv lenght 6918144\n",
            "output csv image count:  1689.0\n",
            "number of expected images 1689\n",
            "CPU times: user 4min 55s, sys: 14.7 s, total: 5min 9s\n",
            "Wall time: 5min 12s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "print('we are making eval csv')\n",
        "make_df_save_csv('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu3jlgVMRPmy",
        "outputId": "9c3a6a24-5638-4a5b-d24a-c04477d198ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we are making eval csv\n",
            "started tf record:  next_day_wildfire_spread_train_00.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_00.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_01.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_01.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_02.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_02.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_03.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_03.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_04.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_04.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_05.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_05.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_06.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_06.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_07.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_07.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_08.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_08.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_09.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_09.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_10.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_10.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_11.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_11.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_12.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_12.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_13.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_13.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_14.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_14.tfrecord\n",
            "csv output is complete\n",
            "output csv lenght 61353984\n",
            "output csv image count:  14979.0\n",
            "number of expected images 14979\n",
            "CPU times: user 44min 20s, sys: 2min 16s, total: 46min 36s\n",
            "Wall time: 47min 29s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "print('we are making eval csv')\n",
        "make_df_save_csv('train')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO/O+EU/hGyz9ucafPTyp8D",
      "collapsed_sections": [
        "hD7ndnzBQ2A1"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1ud4nysR8AygsWvB0kf3c1Jaz4RlN_q08",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
